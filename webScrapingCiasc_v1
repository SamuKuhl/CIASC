import requests
from bs4 import BeautifulSoup
import sqlite3

def scrape_page(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'lxml')
    links = [link.get('href') for link in soup.find_all('a', href=True)]
    return links
def save_links_to_database(links):
    conn = sqlite3.connect('links.db')
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS links (url TEXT)''')
    for link in links:
        c.execute("INSERT INTO links (url) VALUES (?)", (link,))
    conn.commit()
    conn.close()
start_url = "http://ciasc.sc.gov.br"
urls_to_visit = [start_url]
visited_urls = set()
while urls_to_visit:
    current_url = urls_to_visit.pop(0)
    if current_url not in visited_urls:
        print("Scraping:", current_url)
        links = scrape_page(current_url)
        save_links_to_database(links)
        visited_urls.add(current_url)
        for link in links:
            if link.startswith("http") and "ciasc.sc.gov.br" in link:
                urls_to_visit.append(link)
print("Crawler conclu√≠do.")
